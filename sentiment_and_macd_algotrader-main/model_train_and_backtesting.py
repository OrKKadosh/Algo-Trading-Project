# -*- coding: utf-8 -*-
"""Model Train and Backtesting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I2uhSInccsArBRQB517J-PeH7-N-mEbP

#User Defines
"""

start_date = '2022-06-05'
end_date = '2023-06-05'

#0.5
percent_to_target = 0.001
max_nan_count = 260
#252 = 36 stocks

risk_free_rate = 0.002


start_train = '2022-06-05'
end_train = '2023-03-05'

start_test = '2023-03-05'
end_test = '2023-06-05'

#FileNames
earnings_file_name = "All_Earnings_Grades.csv"
daily_MACD_file_name = f"Daily_Optimized_MACD_{start_date}_{end_date}.csv"
twits_file_name = "merged_twits.csv"
news_sentiments_file_name = "newsTrainSentimentScoreSP500.csv"
OHLC_file_name = 'daily_OHLC_data.csv'

"""#Includes (Can be hidden)"""

# ! pip3 install pandas
# ! pip3 install backtesting

import pandas as pd
import numpy as np
from datetime import datetime, date, timedelta
import matplotlib.pyplot as plt

"""#Load files into dataframes and fix them into one data frame (Can be hidden)"""

earnings_df = pd.read_csv(earnings_file_name)
optimized_MACD_df = pd.read_csv(daily_MACD_file_name)
twits_grades_df = pd.read_csv(twits_file_name)
news_sentiment_df = pd.read_csv(news_sentiments_file_name)
daily_OHLC_df = pd.read_csv(OHLC_file_name)

def fix_earnings_df(earnings_df, expand = False, delete_zeros = True):
  fixed_df = earnings_df.copy()
  if expand:
    df = fixed_df.copy()
    df['reportedDate'] = pd.to_datetime(df['reportedDate'])
    expanded_df = pd.DataFrame()

    for symbol, group in df.groupby('Symbol'):
        date_range = pd.date_range(start=group['reportedDate'].min(), end=group['reportedDate'].max(), freq='D')
        expanded_group = pd.DataFrame({'reportedDate': date_range})
        expanded_group['Symbol'] = symbol
        expanded_group = expanded_group.merge(group[['reportedDate', 'Grade']], on='reportedDate', how='left')
        expanded_group['Grade'] = expanded_group['Grade'].ffill()
        expanded_df = pd.concat([expanded_df, expanded_group])

    expanded_df = expanded_df.reset_index(drop=True)
    fixed_df = expanded_df.copy()


  fixed_df = fixed_df[(fixed_df['reportedDate'] >= start_date) & (fixed_df['reportedDate'] <= end_date)]

  if delete_zeros:
    #print("Deleted Zeros:\n" + str(fixed_df[fixed_df['Grade'] == 0]))
    fixed_df = fixed_df[fixed_df['Grade'] != 0]
  fixed_df["Earnings Grade"] = fixed_df['Grade']
  #fixed_df["Date"] = fixed_df['reportedDate']
  fixed_df['Date'] = pd.to_datetime(fixed_df['reportedDate'], format='%Y-%m-%d').dt.date
  return fixed_df[['Symbol', 'Date', 'Earnings Grade']]

#hype_normalized_for parameter: how you want to calculate Total Hype relative for hype_normalized_for.
#for example: hype_normalized_for=['Day', 'stock'] will calculate the hype relative for each stock, in each day. ['All'] it's calculate in relative to whole table.

#drop_zeros should be True because we dont want calculate non relevant posts
def fix_twits_grades_df(twits_grades_df, drop_zeros = True, hype_normalized_for = ['All']):
  fixed_df = twits_grades_df.copy()
  if drop_zeros:
    fixed_df = fixed_df[fixed_df['grade'] != 0]

  fixed_df['Day'] = pd.to_datetime(fixed_df['date']).dt.date
  fixed_df['All'] = 0

  fixed_df['Total Hype'] = np.nanmean([
    fixed_df['Likes'] / fixed_df.groupby(hype_normalized_for)['Likes'].transform('mean'),
    fixed_df['Replies'] / fixed_df.groupby(hype_normalized_for)['Replies'].transform('mean'),
    fixed_df['Retweets'] / fixed_df.groupby(hype_normalized_for)['Retweets'].transform('mean'),
    fixed_df['Views'] / fixed_df.groupby(hype_normalized_for)['Views'].transform('mean')
  ], axis=0)

  fixed_df['Weighted Grade'] = fixed_df['grade'] * fixed_df['Total Hype']
  fixed_df['Day Final Grade'] = fixed_df.groupby(['Day', 'stock'])['Weighted Grade'].transform('sum')

  output_df = pd.DataFrame({'Symbol': fixed_df['stock'], 'Date': fixed_df['Day'], 'Social Grade': fixed_df['Day Final Grade']})
  output_df = output_df.drop_duplicates(subset='Date', keep='first')

  return output_df

def fix_news_sentiment_df(news_sentiment_df):
  fixed_df = news_sentiment_df.copy()
  fixed_df = fixed_df[(fixed_df['Start Date'] != '0') & (fixed_df['News Sentiment Score'] != 0)]
  fixed_df['Date'] = pd.to_datetime(fixed_df['Start Date'], format='%Y%m%dT%H%M').dt.date
  return fixed_df[['Symbol', 'Date', 'News Sentiment Score']]

def fix_optimized_MACD_df(optimized_MACD_df):
  fixed_df = optimized_MACD_df.copy()
  fixed_df['Symbol'] = fixed_df['stock']
  fixed_df['Date'] = pd.to_datetime(fixed_df['Date'], format='%Y-%m-%d').dt.date
  return fixed_df[['Symbol', 'Date', 'Optimized MACD']]

def fix_daily_OHLC_df(daily_OHLC_df):
  fixed_df = daily_OHLC_df.copy()
  fixed_df['Date'] = pd.to_datetime(fixed_df['Date'])
  fixed_df['Week'] = fixed_df['Date'].dt.isocalendar().week + fixed_df['Date'].dt.year * 100
  fixed_df['Weekly Close'] = fixed_df.groupby(['stock', 'Week'])['Close'].transform('last')
  fixed_df['Returns Till Weekend'] = (fixed_df['Weekly Close'] - fixed_df['Close'])/fixed_df['Close']
  #fixed_df['target'] = 1 if (fixed_df['Returns Till Weekend'] >= percent_to_target/100) else -1 if (fixed_df['Returns Till Weekend'] <= percent_to_target/-100) else 0
  fixed_df['target'] = np.where(fixed_df['Returns Till Weekend'] >= percent_to_target/100, 2, np.where(fixed_df['Returns Till Weekend'] <= percent_to_target/-100, 0, 1))
  fixed_df['Symbol'] = fixed_df['stock']
  fixed_df['Date'] = pd.to_datetime(fixed_df['Date']).dt.date
  return fixed_df[['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'target']]

"""#Screen the Most Hyped Stocks"""

fixed_earnings_df = fix_earnings_df(earnings_df, expand = False, delete_zeros = True)
fixed_twits_grades_df = fix_twits_grades_df(twits_grades_df, True, ['Day', 'stock'])
fixed_news_sentiment_df = fix_news_sentiment_df(news_sentiment_df)
fixed_Optimized_MACD_df = fix_optimized_MACD_df(optimized_MACD_df)
fixed_daily_OHLC_df = fix_daily_OHLC_df(daily_OHLC_df)
target_df = fixed_daily_OHLC_df[['Symbol', 'Date', 'target']]

#always fixed_Optimized_MACD_df first!!!
merged_df = pd.merge(fixed_Optimized_MACD_df, fixed_twits_grades_df, on=['Symbol', 'Date'], how='left')
merged_df = pd.merge(merged_df, fixed_news_sentiment_df, on=['Symbol', 'Date'], how='left')
merged_df = pd.merge(merged_df, fixed_earnings_df, on=['Symbol', 'Date'], how='left')
merged_df = pd.merge(merged_df, target_df, on=['Symbol', 'Date'], how='left')
merged_df = merged_df.sort_values(by=['Symbol', 'Date'])

train_df = merged_df
train_df['Date'] = pd.to_datetime(train_df['Date'])
symbols_we_have = set(fixed_news_sentiment_df['Symbol'].values)
print(f"Symbols we have full data on: {len(symbols_we_have)}")

symbols_nan_count = {}
for ticker in symbols_we_have:
  symbols_nan_count[ticker] = merged_df[merged_df['Symbol'] == ticker][['Social Grade','News Sentiment Score']].isna().sum().sum()

symbols_nan_sorted = dict(sorted(symbols_nan_count.items(), key=lambda x: x[1]))

most_hyped_dict = dict()
for key, value in symbols_nan_sorted.items():
  if value <= max_nan_count:
    most_hyped_dict[key] = value

most_hyped_stocks = list(most_hyped_dict.keys())
most_hyped_dict

#First option: Social Grade OR News Sentiment Score OR Earnings Grade != NaN
#train_df = train_df[(train_df['Symbol'].isin(symbols_we_have)) & ((train_df['Earnings Grade'].notna()) | (train_df['News Sentiment Score'].notna()) | (train_df['Social Grade'].notna()))]
#train_df = train_df[((train_df['Earnings Grade'].notna()) | (train_df['News Sentiment Score'].notna()) | (train_df['Social Grade'].notna()))]

#Second option: Social Grade AND News Sentiment Score != NaN
#train_df = train_df[(train_df['Symbol'].isin(symbols_we_have)) & (train_df['News Sentiment Score'].notna()) & (train_df['Social Grade'].notna())]

#Third option: Social Grade OR News Sentiment Score OR Earnings Grade != NaN
#train_df = train_df[(train_df['Symbol'].isin(symbols_we_have)) | (train_df['News Sentiment Score'].notna()) | (train_df['Social Grade'].notna()))]
#train_df = train_df[((train_df['News Sentiment Score'].notna()) | (train_df['Social Grade'].notna()))]

#4th option: train on all data for most hyped stocks
train_df = train_df[(train_df['Symbol'].isin(most_hyped_stocks))]

#df
#print(f'''
#=====================================================\n fixed_earnings_df:\n{fixed_earnings_df}\n
#=====================================================\n fixed_twits_grades_df:\n{fixed_twits_grades_df}\n
#=====================================================\n fixed_news_sentiment_df:\n{fixed_news_sentiment_df}\n
#=====================================================\n fixed_Optimized_MACD_df:\n{fixed_Optimized_MACD_df}\n
#=====================================================\n merged_df:\n{merged_df}\n
#''')

train_df[(train_df['Symbol'] == 'AAPL') & (train_df['Date'] < start_test)]

"""#Train The Model"""

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

start_train = pd.Timestamp(start_train)
end_train = pd.Timestamp(end_train)

start_test = pd.Timestamp(start_test)
end_test = pd.Timestamp(end_test)

features_columns = ['Optimized MACD',	'Social Grade',	'News Sentiment Score',	'Earnings Grade']
target_column = ['target']
df = train_df

X_train = train_df[(train_df['Date'] >= start_train) & (train_df['Date'] <= end_train)][features_columns]
y_train = train_df[(train_df['Date'] >= start_train) & (train_df['Date'] <= end_train)][target_column]

X_test = train_df[(train_df['Date'] >= start_test) & (train_df['Date'] <= end_test)][features_columns]
y_test = train_df[(train_df['Date'] >= start_test) & (train_df['Date'] <= end_test)][target_column]

model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)
model.fit(X_train, y_train)
accuracy = model.score(X_test, y_test)

'''import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier

# Step 1: Split data into features (X) and target variable (y)
X = df.drop('target', axis=1)
y = df['target']

# Step 2: Handle missing values
X = X.fillna(0)  # Replace missing values with 0, or use other imputation techniques

# Step 3: Convert categorical variables if necessary
X['Symbol'] = X['Symbol'].astype('category').cat.codes  # Convert 'Symbol' column to categorical and encode as numerical
X['Date'] = pd.to_datetime(X['Date']).astype(int)  # Convert 'Date' column to datetime and encode as numerical

# Step 4: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Adjust the test_size and random_state as needed

# Step 5: Initialize XGBoost model with desired hyperparameters
model = XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)  # Adjust the hyperparameters as needed

# Step 6: Fit the XGBoost model on the training data
model.fit(X_train, y_train)

# Step 7: Evaluate the performance of the trained model on the testing data
accuracy = model.score(X_test, y_test)
print("Accuracy:", accuracy)
'''

"""#Backtesting"""

def get_model_prediction(model, Optimized_MACD,	Social_Grade,	News_Sentiment_Score,	Earnings_Grade):
  single_df = {'Optimized MACD': [Optimized_MACD],
           'Social Grade': [Social_Grade],
           'News Sentiment Score': [News_Sentiment_Score],
           'Earnings Grade': [Earnings_Grade]}
  single_df = pd.DataFrame(single_df)
  return model.predict(single_df)[0]

get_model_prediction(model, 1,1,1,5)

def plot_return_values(model, social=True, news=True, earnings=True, num_points = 30, dot_size = 4):

    x_values = np.linspace(-1, 1, num_points)
    y_values = np.linspace(-1, 1, num_points)
    colors = ['red', 'orange', 'green']

    fig, ax = plt.subplots()

    for x in x_values:
        for y in y_values:
            return_value = get_model_prediction(model, x, y if social else np.nan, y if news else np.nan, y if earnings else np.nan)
            ax.scatter(x, y, color=colors[return_value], s=dot_size)

    ax.set_title("Return Values")
    ax.set_xlabel("Optimized MACD")
    ax.set_ylabel("Sentiment")

    ax.set_xlim(-1, 1)
    ax.set_ylim(-1, 1)

    plt.show()

def get_regular_return(OHLC_data, ticker):
  test_period_ticker_df = OHLC_data[OHLC_data['Symbol'] == ticker]
  open = test_period_ticker_df['Open'].values[0]
  close = test_period_ticker_df['Close'].values[-1]
  #print(f"open: {open} || close: {close} || return: {(close-open)/open}\ndf:\n{test_period_ticker_df}")
  return (close-open)/open

def get_portfolio_regular_returns(OHLC_data, portfolio, start, end):
  returns = []
  test_period_df = OHLC_data[(OHLC_data['Date'] >= start) & (OHLC_data['Date'] <= end)]
  for ticker in portfolio:
    returns.append(get_regular_return(test_period_df, ticker))
  return sum(returns)/len(returns)

from backtesting import Backtest, Strategy
import backtesting as bt
from backtesting.lib import crossover

from backtesting.test import SMA, GOOG
from statistics import mean, stdev
from backtesting.lib import resample_apply
#import pandas_ta as ta
#
#def indicator(data):
#  bbands = ta.bbands(close = data.Close.s, std = 1)
#  return bbands.to_numpy().T[:3]



'''def init_current_dfs(stock_ticker, stock_OHLC_df, stock_features_df):
  global current_ticker, current_OHLC_df, current_features_df
  current_ticker = stock_ticker
  current_OHLC_df = stock_OHLC_df
  current_features_df = stock_features_df'''

class SentimentStrategy(Strategy):

    ticker = None
    OHLC_df = None
    features_df = None
    model = None

    def init(self):
        close = self.data.Close
        self.closes = self.I(SMA, close, 1)
        self.dates = list(self.OHLC_df['Date'].values)



    def next(self):
        i = len(self.closes)-2
        curr_date = self.dates[i]
        curr_date = pd.to_datetime(curr_date)
        curr_MACD = self.features_df[self.features_df['Date'] == curr_date]['Optimized MACD'].values
        if(len(curr_MACD) > 0): curr_MACD = curr_MACD[0]
        else: curr_MACD = np.nan

        curr_Social = self.features_df[self.features_df['Date'] == curr_date]['Social Grade'].values
        if(len(curr_Social) > 0): curr_Social = curr_Social[0]
        else: curr_Social = np.nan

        curr_News = self.features_df[self.features_df['Date'] == curr_date]['News Sentiment Score'].values
        if(len(curr_News) > 0): curr_News = curr_News[0]
        else: curr_News = np.nan

        curr_Earnings = self.features_df[self.features_df['Date'] == curr_date]['Earnings Grade'].values
        if(len(curr_Earnings) > 0): curr_Earnings = curr_Earnings[0]
        else: curr_Earnings = np.nan

        #print(f"\nself.dates[{i}] = {self.dates[i]}. Features: curr_MACD: {curr_MACD},	curr_Social: {curr_Social}, curr_News:	{curr_News}, curr_Earnings:	{curr_Earnings}")
        #print(self.features_df)
        weekend = (self.dates[i].weekday() == 4) or (i+1 < len(self.dates) and self.dates[i+1].weekday() == 0) #last condition is fix for long weekend because vacations
        if self.position:
          if(get_model_prediction(self.model, curr_MACD,	curr_Social,	curr_News,	curr_Earnings) == 0) or weekend:
            print(f"{self.dates[i]} {self.ticker}: Closed position. Reason: {'Weekend' if (self.dates[i].weekday() == 4) or (i+1 < len(self.dates) and self.dates[i+1].weekday() == 0) else f'Model Sell Signal for: Optimized MACD: {curr_MACD:.3f},	Social Sentiment: {curr_Social:.3f}, News Sentiment:	{curr_News:.3f}, Earnings Grade:	{curr_Earnings}'}\n")
            self.position.close()

        else:
          if get_model_prediction(self.model, curr_MACD,	curr_Social,	curr_News,	curr_Earnings) == 2 and not weekend:
            print(f"{self.dates[i]} {self.ticker}: Opened position. Reason: Model Buy Signal for: Optimized MACD: {curr_MACD:.3f},	Social Sentiment: {curr_Social:.3f}, News Sentiment:	{curr_News:.3f}, Earnings Grade:	{curr_Earnings}")
            self.buy()


class SmaCross(Strategy):
    n1 = 10
    n2 = 20

    def init(self):
        close = self.data.Close
        self.sma1 = self.I(SMA, close, self.n1)
        self.sma2 = self.I(SMA, close, self.n2)

    def next(self):
        if crossover(self.sma1, self.sma2):
            self.buy()
        elif crossover(self.sma2, self.sma1):
            self.sell()




def calculate_portfolio_stats(all_data, portfolio_stocks, print_stocks, features_df):
    '''
        all_data: pd.Dataframe, needs to include all your required stocks OHLCV data.
        portfolio_stocks: List[str], list of your required portfolio stock tickers for an example: ['AAPL','TSLA'].
        must be consistentr with the stocks column uin your all_data df
    '''
    assert len(portfolio_stocks) > 1, 'portfolio must include more than one stock'
    returns_ann = []
    returns = []
    sharpes = []
    volatilities = []
    max_drawdowns = []
    original_returns = []
    for stock in portfolio_stocks:
        if stock == 'A': continue
        #init_current_dfs(stock, all_data[all_data['Symbol'] == stock], features_df[features_df['Symbol'] == stock])
        all_data['dt'] = pd.to_datetime(all_data['Date'])
        all_data.set_index('dt', inplace = True)
        #print(all_data)
        stock_data = all_data[all_data['Symbol'] == stock][['Open', 'High', 'Low', 'Close', 'Volume']]
        sentiment_strategy = SentimentStrategy
        sentiment_strategy.ticker = stock
        sentiment_strategy.OHLC_df = all_data[all_data['Symbol'] == stock]
        sentiment_strategy.features_df = features_df[features_df['Symbol'] == stock]
        sentiment_strategy.features_df['Date'] = pd.to_datetime(sentiment_strategy.features_df['Date'])
        sentiment_strategy.model = model
        bt = Backtest(stock_data, sentiment_strategy,
                    cash=10000, commission=.002,
                    exclusive_orders=True)
        output = bt.run()
        if(output.loc['Sharpe Ratio'] != "NaN" and output.loc['Return [%]'] != "NaN" and output.loc['Volatility (Ann.) [%]'] != "NaN" and output.loc['Max. Drawdown [%]'] != "NaN"):
          #if(pd.notna(output.loc['Sharpe Ratio'])):
          #  sharpes.append(output.loc['Sharpe Ratio'])
          if(pd.notna(output.loc['Return (Ann.) [%]']) and pd.notna(output.loc['Volatility (Ann.) [%]'])and output.loc['Volatility (Ann.) [%]'] != 0):
            sharpes.append((output.loc['Return (Ann.) [%]'] - risk_free_rate)/output.loc['Volatility (Ann.) [%]'])
            print("my calculated sharpe for {}: {:.2f}. their sharpe: {:.2f}".format(stock, (output.loc['Return (Ann.) [%]'] - risk_free_rate)/output.loc['Volatility (Ann.) [%]'], output.loc['Sharpe Ratio']))
          if(pd.notna(output.loc['Return (Ann.) [%]'])):
            returns_ann.append(output.loc['Return (Ann.) [%]'])
          if(pd.notna(output.loc['Return [%]'])):
            returns.append(output.loc['Return [%]'])
          volatilities.append(output.loc['Volatility (Ann.) [%]'])
          max_drawdowns.append(output.loc['Max. Drawdown [%]'])
          #original_returns.append(test_df[test_df['Symbol'] == stock][[Y_TARGET]].iloc[0][Y_TARGET])
        #if print_stocks:
        #  print("Feature: "+str(test_df[test_df['Symbol'] == stock][[OUR_FEATURE]].iloc[0][OUR_FEATURE])+" || Return: "+str(output.loc['Return [%]'])+" || Return without the algorithm: "+str(test_df[test_df['Symbol'] == stock][[Y_TARGET]].iloc[0][Y_TARGET])+" || Sharpe: "+str(output.loc['Sharpe Ratio']) + " || Symbol: " + stock)
    mean_returns = mean(returns)
    mean_returns_ann = mean(returns_ann)
    returns_std = stdev(returns)
    mean_sharpes = mean(sharpes)
    sharpes_std = stdev(sharpes)
    mean_volatilities = mean(volatilities)
    mean_max_drawdowns = mean(max_drawdowns)
    return mean_returns, mean_returns_ann, returns_std, mean_sharpes, sharpes_std, mean_volatilities, mean_max_drawdowns

def calculate_stock_stats(all_data, stock, features_df):
    '''
        all_data: pd.Dataframe, needs to include all your required stocks OHLCV data.
        portfolio_stocks: List[str], list of your required portfolio stock tickers for an example: ['AAPL','TSLA'].
        must be consistentr with the stocks column uin your all_data df
    '''
    #print(all_data)
    all_data['dt'] = pd.to_datetime(all_data['Date'])
    all_data.set_index('dt', inplace = True)
    #print(all_data)
    stock_data = all_data[all_data['Symbol'] == stock][['Open', 'High', 'Low', 'Close', 'Volume']]
    sentiment_strategy = SentimentStrategy
    sentiment_strategy.ticker = stock
    sentiment_strategy.OHLC_df = all_data[all_data['Symbol'] == stock]
    sentiment_strategy.features_df = features_df[features_df['Symbol'] == stock]
    sentiment_strategy.features_df['Date'] = pd.to_datetime(sentiment_strategy.features_df['Date'])
    sentiment_strategy.model = model
    bt = Backtest(stock_data, sentiment_strategy,
                cash=10000, commission=.002,
                exclusive_orders=True)
    output = bt.run()
    return output

def print_portfolio_results(portfolio_results):
  mean_returns, mean_returns_ann, returns_std, mean_sharpes, sharpes_std, mean_volatilities, mean_max_drawdowns = portfolio_results
  print('''Portfolio results:
Mean Returns: {:.2f}%
Mean Returns (Ann): {:.2f}%
Returns (Ann) std: {:.2f}
Sharpes std: {:.2f}
Mean Volatilities (Ann): {:.2f}%
Mean Max Drawdowns: {:.2f}%

Portfolio Calculated Sharpe: {:.2f}
'''.format(mean_returns, mean_returns_ann, returns_std, sharpes_std, mean_volatilities, mean_max_drawdowns, (mean_returns_ann - risk_free_rate)/mean_volatilities))
  regular_returns = (get_portfolio_regular_returns(fixed_daily_OHLC_df, most_hyped_stocks, start_test, end_test)*100)
  is_not = "" if mean_returns > regular_returns else "not "
  print(f"Mean returns for our stocks without the model \nwas {regular_returns:.2f}%, and when using our model was {mean_returns:.2f}%")
  print(f"The ratio between using our model and \nnot using our model is {abs(mean_returns/regular_returns):.2f}.\nThat means that this is {is_not}better to use our model on our portfolio stocks.")

backtesting_OHLC_df = fixed_daily_OHLC_df[(fixed_daily_OHLC_df['Date'] >= start_test) & (fixed_daily_OHLC_df['Date'] <= end_test)]
backtesting_features_df = train_df[(train_df['Date'] >= start_test) & (train_df['Date'] <= end_test)]

#backtesting_OHLC_df = backtesting_OHLC_df.set_index('Date', inplace=True)
#backtesting_features_df.index = backtesting_features_df['Date']
#backtesting_features_df['Date'] = pd.to_datetime(backtesting_features_df['Date'])

calculate_stock_stats(backtesting_OHLC_df, 'AAL', backtesting_features_df)

portfolio_results = calculate_portfolio_stats(backtesting_OHLC_df, most_hyped_stocks , True, backtesting_features_df)

"""#Results"""

plot_return_values(model, True, True, False, 30, 10)

import warnings

# Suppress FutureWarning
warnings.simplefilter(action='ignore', category=FutureWarning)

#print(f"Target 2: {len(train_df[train_df['target'] == 2].values)} || Target 1: {len(train_df[train_df['target'] == 1].values)} || Target 0: {len(train_df[train_df['target'] == 0].values)}")
#print("")
#print("Model Stats:\n")
#print("Feature Importances:")
#for i in range(len(features_columns)):
#  print(f"{features_columns[i]}: {int(model.feature_importances_[i]*100)}%")
#print("")
#print("Accuracy:", accuracy)
#print("")

print_portfolio_results(portfolio_results)
print("")
print(f"Stocks we checked ({len(most_hyped_stocks)}):\n{most_hyped_dict}")